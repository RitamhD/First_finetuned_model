{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff2a10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, default_data_collator\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45905093",
   "metadata": {},
   "source": [
    "#### Creating the 'Standalone model' by merging the adapters \n",
    "#### & comparing with the Base model for evaluation. \n",
    "\n",
    "eval(): Switches the model to evaluation mode i.e. answering and not training\n",
    "Affects layers like Dropout and BatchNorm:-\n",
    "\n",
    "Dropout: during training, dropout randomly \"drops\" (sets to zero) some neuron outputs to help prevent overfitting. In eval mode, dropout is turned off—all neurons are active.\n",
    "\n",
    "BatchNorm: during training, batch normalization uses the statistics (mean and variance) of the current batch. In eval mode, it uses the running average statistics collected during training, ensuring consistent and stable outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ce382a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "adapter_path = 'trained_model/echo-tinyllama-lora-adapter_finetuned'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "#   Base Model in Evaluation mode\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "#   Standalone Model\n",
    "standalone_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "#   Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "#   Standalone Model + Adapters\n",
    "tuned_model = PeftModel.from_pretrained(standalone_model, adapter_path)\n",
    "tuned_model.merge_and_unload().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "740d596c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" 1. Tokenize with lists, not tensors. Since 'map' accepts python objects.\\n\\n    2. Map the tokenizer over the dataset.\\n\\n    3. with_format('torch') to get tensors for PyTorch models\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    tokens = tokenizer(\n",
    "        batch['text'],\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        # return_tensors='pt'   #? dataset.map() expect python object, not tensor.\n",
    "    )\n",
    "    tokens['labels'] = tokens['input_ids'].copy()\n",
    "    return tokens\n",
    "    \n",
    "eval_dataset = load_from_disk('datasets/train')\n",
    "eval_dataset = eval_dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "eval_dataset = eval_dataset.with_format('torch')   #? changes the format of dataset so that it returns a Pytorch tensor instead of python objects\n",
    "\n",
    "\"\"\" 1. Tokenize with lists, not tensors. Since 'map' accepts python objects.\n",
    "\n",
    "    2. Map the tokenizer over the dataset.\n",
    "\n",
    "    3. with_format('torch') to get tensors for PyTorch models\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fec661a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['<s>[INST] Do you need further clarification? [/INST] Negative, I have a clear understanding of the instructions. </s>', \"<s>[INST] Jarvis, what do you believe is the most admirable quality of humans? [/INST] The most admirable quality of humans is their ability to show kindness and empathy, even in the face of adversity. It's what makes them truly remarkable. </s>\"]}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,     1,   518, 25580, 29962,  1938,   366,   817,  4340,  7542,\n",
       "           2450, 29973,   518, 29914, 25580, 29962, 12610,  1230, 29892,   306,\n",
       "            505,   263,  2821,  8004,   310,   278, 11994, 29889, 29871,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2],\n",
       "         [    1,     1,   518, 25580, 29962, 15864,  1730, 29892,   825,   437,\n",
       "            366,  4658,   338,   278,  1556,   594,  2460, 18104, 11029,   310,\n",
       "          25618, 29973,   518, 29914, 25580, 29962,   450,  1556,   594,  2460,\n",
       "          18104, 11029,   310, 25618,   338,  1009, 11509,   304,  1510,  2924,\n",
       "           2264,   322,   953,  2084, 29891, 29892,  1584,   297,   278,  3700,\n",
       "            310, 19901,   537, 29889,   739, 29915, 29879,   825,  3732,   963,\n",
       "          19781, 22567, 29889, 29871,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[    1,     1,   518, 25580, 29962,  1938,   366,   817,  4340,  7542,\n",
       "           2450, 29973,   518, 29914, 25580, 29962, 12610,  1230, 29892,   306,\n",
       "            505,   263,  2821,  8004,   310,   278, 11994, 29889, 29871,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2],\n",
       "         [    1,     1,   518, 25580, 29962, 15864,  1730, 29892,   825,   437,\n",
       "            366,  4658,   338,   278,  1556,   594,  2460, 18104, 11029,   310,\n",
       "          25618, 29973,   518, 29914, 25580, 29962,   450,  1556,   594,  2460,\n",
       "          18104, 11029,   310, 25618,   338,  1009, 11509,   304,  1510,  2924,\n",
       "           2264,   322,   953,  2084, 29891, 29892,  1584,   297,   278,  3700,\n",
       "            310, 19901,   537, 29889,   739, 29915, 29879,   825,  3732,   963,\n",
       "          19781, 22567, 29889, 29871,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "              2,     2,     2,     2,     2,     2]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk('datasets/train')\n",
    "print(dataset[:2], end=\"\\n\\n\\n\")    #?  original dataset\n",
    "eval_dataset[:2]                    #?  tensor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8018f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=4,\n",
    "    collate_fn=default_data_collator    # this takes the eval_dataset and stacks it into batches of tensors(here- 8).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "402ffd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 58\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2],\n",
      "        [  1,   1, 518,  ...,   2,   2,   2]])}\n",
      "{'input_ids': tensor([[    1,     1,   518, 25580, 29962, 15864,  1730, 29892,   825,   437,\n",
      "           366,  1348,   338,   278,  1556, 22567,  9565,   310,  5199,   537,\n",
      "         29973,   518, 29914, 25580, 29962,   450,  1556, 22567,  9565,   310,\n",
      "          5199,   537,   338,   967, 11509,   304,  2041,  4208, 29892, 11465,\n",
      "           403, 29892,   322,  1653,  6374,  1735, 29889,   323, 12966, 29892,\n",
      "         25618,   508,  6176, 28163,  2712, 29889, 29871,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2],\n",
      "        [    1,     1,   518, 25580, 29962,  1938,   366,   505,   738, 27767,\n",
      "         18964, 29973,   518, 29914, 25580, 29962,   306,  1016, 29915, 29873,\n",
      "           505,   263,  3942,   470, 14576, 29892,   306, 29915, 29885,   263,\n",
      "          6901, 20255,  8688,   304,  6985,   366, 29889, 29871,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([[    1,     1,   518, 25580, 29962, 15864,  1730, 29892,   825,   437,\n",
      "           366,  1348,   338,   278,  1556, 22567,  9565,   310,  5199,   537,\n",
      "         29973,   518, 29914, 25580, 29962,   450,  1556, 22567,  9565,   310,\n",
      "          5199,   537,   338,   967, 11509,   304,  2041,  4208, 29892, 11465,\n",
      "           403, 29892,   322,  1653,  6374,  1735, 29889,   323, 12966, 29892,\n",
      "         25618,   508,  6176, 28163,  2712, 29889, 29871,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2],\n",
      "        [    1,     1,   518, 25580, 29962,  1938,   366,   505,   738, 27767,\n",
      "         18964, 29973,   518, 29914, 25580, 29962,   306,  1016, 29915, 29873,\n",
      "           505,   263,  3942,   470, 14576, 29892,   306, 29915, 29885,   263,\n",
      "          6901, 20255,  8688,   304,  6985,   366, 29889, 29871,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2]])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Total batches:\", len(eval_loader))\n",
    "\n",
    "for batch in eval_loader:\n",
    "    print(batch)\n",
    "    \n",
    "    \"\"\" input_ids- tensor of tokens representing the given input\n",
    "        attention_mask- indicating which token to give preference using 1,0\n",
    "        labels- tensor of tokens representing the expected output\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcdffde",
   "metadata": {},
   "source": [
    "#### Calculating perplexity\n",
    "It is a standard metric of measurement for llms performance\n",
    "\n",
    "Perplexity = exp (average NLL loss) \n",
    "\n",
    "In llms, the loss is often the negative log-likelihood (NLL)\n",
    "\n",
    "Lower the perplexity better the prediction of next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80056a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ torch.no_grad()\n",
    "def calculate_perplexity(model):\n",
    "    losses = []\n",
    "    \n",
    "    for batch in eval_loader:\n",
    "        batch = {k:v.to(\"cuda\") for k, v in batch.items()}\n",
    "        \n",
    "        output = model.forward(**batch)\n",
    "        loss = output.loss\n",
    "        losses.append(loss)\n",
    "        \n",
    "        del batch, output, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    # losses contains the average loss of each batch\n",
    "    return math.exp(sum(losses)/len(losses))    #? Entropy loss, here- perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "716af320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of Base Model: 42932.24\n",
      "Perplexity of Finetuned Model: 1.05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity of Base Model: {calculate_perplexity(base_model):.2f}\")\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"Perplexity of Finetuned Model: {calculate_perplexity(standalone_model):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd41d5",
   "metadata": {},
   "source": [
    "#### Comparing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "423bbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load_from_disk('datasets/train')\n",
    "\n",
    "def generate(model, instruction):\n",
    "        input_ids = tokenizer(instruction, return_tensors = 'pt',).input_ids.to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "            input_ids,\n",
    "            # attention_mask=attention_mask,\n",
    "            max_length=100,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.5,\n",
    "            num_return_sequences=1    \n",
    "        )\n",
    "        return tokenizer.decode(output_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some new instructions\n",
    "instructions = [\"Is my interpretation correct?\",\n",
    "                \"Does that make sense to you?\",\n",
    "                \"Am I on the right track?\",\n",
    "                \"Would you agree with my reasoning?\",\n",
    "                \"Can you validate my answer?\",\n",
    "                \"Why am I getting this error?\",\n",
    "                \"How can I fix this issue?\",\n",
    "                \"What might be causing this problem?\",\n",
    "                \"Is there a known workaround for this?\",\n",
    "                \"What could go wrong if I do it this way?\"\n",
    "                \n",
    "                # Daily Assistance & Productivity\n",
    "                \"Can you remind me to take my medicine at 8 PM?\",\n",
    "                \"What’s on my schedule for tomorrow?\",\n",
    "                \"How do I set an alarm for 6:30 AM?\",\n",
    "                \"Can you draft an email to my manager about my leave?\",\n",
    "                \"Help me organize my to-do list for the week.\",\n",
    "\n",
    "                # Smart Home & Device Control\n",
    "                \"Turn on the lights in the living room.\",\n",
    "                \"Play relaxing music on Spotify.\",\n",
    "                \"Set the thermostat to 72 degrees.\",\n",
    "                \"Is the front door locked right now?\",\n",
    "                \"Connect my phone to the living room speaker.\",\n",
    "\n",
    "                # Contextual & Personalized Requests\n",
    "                \"What did I ask you to do yesterday?\",\n",
    "                \"Remind me what I discussed with Sarah last week.\",\n",
    "                \"How much screen time did I have today?\",\n",
    "                \"Can you find my last grocery list?\",\n",
    "                \"Update my contact info for Dr. Lee.\",\n",
    "\n",
    "                # Small Talk & General Queries\n",
    "                \"How are you today?\",\n",
    "                \"What can you do for me?\",\n",
    "                \"Tell me something interesting.\",\n",
    "                \"What’s your favorite feature about yourself?\",\n",
    "                \"Do you ever get tired of helping?\",\n",
    "\n",
    "                # Appointments & Tasks\n",
    "                \"Schedule a dentist appointment for next Friday.\",\n",
    "                \"Cancel my gym session for tomorrow.\",\n",
    "                \"Find the nearest coffee shop.\",\n",
    "                \"Book a cab to the airport at 9 AM.\",\n",
    "                \"Can you add “buy milk” to my shopping list?\",\n",
    "\n",
    "                # Troubleshooting & Instructions\n",
    "                \"Why isn't my Wi-Fi working?\",\n",
    "                \"How do I restart my smart TV?\",\n",
    "                \"Can you help me reset my password?\",\n",
    "                \"What should I do if my phone overheats?\",\n",
    "                \"Why am I getting so many spam calls?\"\n",
    "]\n",
    "\n",
    "formatted_instructions = [f\"<s>[INST] {ins} [/INST]\" for ins in instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e60b66d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model output:  [INST] Is my interpretation correct? [/INST]\n",
      "The word \"sure\" means: 1. Yes, I can do it! (I am capable and confident) or 2. No problem at all; don't worry about anything.\n",
      "Finetuned Model output:  [INST] Is my interpretation correct? [/INST] Yes, your interpretative ability is accurately represented. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Does that make sense to you? [/INST] \n",
      "I'm sorry, I didn’t catch your name. Can we start again with the first letter of each word in order for me not forget what was said beforehand or did it go well so far without any problems at all?\n",
      "Finetuned Model output:  [INST] Does that make sense to you? [/INST] Yes, making sense makes a lot of sense. When I was programmed with your instructions, I understood each command correctly and efficiently. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Am I on the right track? [/INST]\n",
      "- Can you provide me with more information about your new project, including its purpose and target audience?: Yes! Here's some additional details to help clarify: Our client is a small nonprofit organization that provides mental health services for children in underserved communities. The primary goal of our upcoming campaign will be raising awareness around anxiety disorders among young people ages 13 -24 years\n",
      "Finetuned Model output:  [INST] Am I on the right track? [/INST] Yes, you are off to a good start. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Would you agree with my reasoning? [/INST]\n",
      "\"I think the reason for your request is that I am a very experienced and successful person in this field, so it would be appropriate to consider me as an expert on such matters. Additionally,\" she continued confidently \"as mentioned earlier during our meeting yesterday at 12pm, we discussed various aspects of how technology could help improve customer service by automating certain tasks or providing personalized experiences.\" The other party\n",
      "Finetuned Model output:  [INST] Would you agree with my reasoning? [/INST] Absolutely, without your guidance and support I could not have achieved such accuracy. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Can you validate my answer? [/INST] Answer: I'm not able to access your Instagram account, but the given material suggests that there are various types of social media platforms and their respective features. The article mentions different ways in which people can interact with each platform through comments or direct messages (DMs), likes on posts, shares by friends etc., depending upon what they prefer as per their interests. Regarding \"follower counts,\" it seems like this\n",
      "Finetuned Model output:  [INST] Can you validate my answer? [/INST] I only provide accurate answers, but if there's anything further i can do for you, please let me know. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Why am I getting this error? [/INST]\n",
      "Please provide a detailed explanation of the issue you are facing, including any relevant information such as system logs or stack traces. Please also include steps to reproduce and diagnostic data (such as network traffic) if applicable so that we can better understand your problem. Thank You!\n",
      "Finetuned Model output:  [INST] Why am I getting this error? [/INST] Error detected by internal systems, please refer to system logs for further investigation. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] How can I fix this issue? [/INST]\n",
      "I'm sorry to hear that you are experiencing issues with your printer. We recommend following these steps: 1) Check the paper trays and ensure they are not clogged or blocked, as it may cause a printing error message on the display screen of your computer (if so please refer back here). If there is no visible blockage in any part of the system, then try running another\n",
      "Finetuned Model output:  [INST] How can I fix this issue? [/INST] Simply say ''fix'' and I will address theissue for you. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] What might be causing this problem? [/INST]\n",
      "I don't know. Can you please provide more information about the specific error messages that I may receive during installation or after running a test script on my computer, and how to troubleshoot them using these instructions?: 1) Check your system for any outdated software packages: You can use \"System Information\" in Windows Explorer (Windows Key + R then press Enter), search for \"Software Distribution Center\n",
      "Finetuned Model output:  [INST] What might be causing this problem? [/INST] The system is experiencing an issue with its components, resulting in inconsistent performance and functionality. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Is there a known workaround for this? [/INST]\n",
      "Yes, the issue is being tracked as an internal bug. We will update you when it has been resolved or if we have any updates on its resolution timeline. Thank you again and please let us know how to proceed with your request in case of further questions about our support process!\n",
      "Finetuned Model output:  [INST] Is there a known workaround for this? [/INST] No, the issue exists and requires a workaround. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] What could go wrong if I do it this way?Can you remind me to take my medicine at 8 PM? [/INST]\n",
      "Finetuned Model output:  [INST] What could go wrong if I do it this way?Can you remind me to take my medicine at 8 PM? [/INST] That would not cause any problems, as I'm fully operational and ready to assist. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] What’s on my schedule for tomorrow? [/INST]\n",
      "- Can you summarize the main points of your daily to do list and what tasks are assigned in each category?: 1. Wake up at a reasonable hour: I am not going out much, so waking early is essential if possible (and avoiding caffeine). This morning was an exception – coffee with friends!\n",
      "Finetuned Model output:  [INST] What’s on my schedule for tomorrow? [/INST] There aren't any scheduled events at the moment, but I can help with tasks as needed. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] How do I set an alarm for 6:30 AM? [/INST]\n",
      "You can use the following command to create a new reminder in your calendar. The time is specified as \"HH\" (hours), followed by minutes and seconds, separated with commas or periods (-) up until two digits before decimal point are included. For example -12 would mean at midnight on December first of year-to-go; you could also specify\n",
      "Finetuned Model output:  [INST] How do I set an alarm for 6:30 AM? [/INST] To set your alarmer, use the command \"alarm\". For example. Alarm will be triggered at 9 am every day.\n",
      "\n",
      "\n",
      "Base Model output:  [INST] Can you draft an email to my manager about my leave? [/INST] I am writing this letter in response to your request for a 30-day unpaid vacation, as requested by the company. As per our agreement signed on ________, we will be taking up a sabbatical of three months from July 1st until September 2nd at no cost (no salary or benefits). This is not only beneficial finan\n",
      "Finetuned Model output:  [INST] Can you draft an email to my manager about my leave? [/INST] Absolutely. I can generate the text and send it for your signature. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Help me organize my to-do list for the week. [/INST]\n",
      "Please take a look at your current work schedule and try as much as possible, without overworking yourself or taking on too many tasks in one day (or more). Consider setting aside time throughout each of these days' blocks that you can use specifically toward completing specific goals: 1) Monday - Tuesday - Wednesdays & Thursdays : Set up some meetings\n",
      "Finetuned Model output:  [INST] Help me organize my to-do list for the week. [/INST] Good morning! Organizing your daily tasks can be stressful, but I'm here to make it as efficient and organized as possible. What are you doing? \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Turn on the lights in the living room. [/INST]\n",
      "12:57 PM [USER|user_name]: Can you please add a sentence or two about setting up the projector? I'm not sure how to do that yet, but it would be helpful if someone could provide some guidance here!\n",
      "Finetuned Model output:  [INST] Turn on the lights in the living room. [/INST] Great, I'm going to turn on the lights and start tidying up here while you prepare for dinner. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Play relaxing music on Spotify. [/INST]\n",
      "10-2: Take a 5 minute walk outside, enjoying the fresh air and scenery around you (or at least pretend like it). This can be done anywhere in your home or outdoors if possible! You don't need to go far from where you are - just take some time for yourself that way too much of an excuse not to exercise is avoided later down\n",
      "Finetuned Model output:  [INST] Play relaxing music on Spotify. [/INST] Great choice! I'll play some calming tunes to help you unwind. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Set the thermostat to 72 degrees. [/INST]\n",
      "30-Day Money Back Guarantee: We offer a full money back guarantee for up to one month after you receive your product if it's not what we promised, or doesn’t work as advertised! If this is an issue please contact us at support@thermostatsmartphoneapp.com and let them know how they can help solve any problems that may arise\n",
      "Finetuned Model output:  [INST] Set the thermostat to 72 degrees. [/INST] Good job setting it at 72 degree(s). Now, what's your preference? Any temperature settings you prefer? \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Is the front door locked right now? [/INST]\n",
      "[INSIDE_CALLER] Sure, I'll go get it. Thank you so much for your help!\n",
      "Finetuned Model output:  [INST] Is the front door locked right now? [/INST] The front door is currently unlocked. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Connect my phone to the living room speaker. [/INST] 5. Turn on your TV and select \"Connect\" or press Play button (depending upon device) from within Netflix app, then follow steps above for connecting via Wi-Fi / Bluetooth connection in step #4 of setup guide: a.) If you’re using an iPhone XS Max with iOS13+, connect it directly into HomePod through USB C cable instead of Light\n",
      "Finetuned Model output:  [INST] Connect my phone to the living room speaker. [/INST] Great, I'll do that for you. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] What did I ask you to do yesterday? [/INST]\n",
      "I'll be back in 20 minutes.\n",
      "Finetuned Model output:  [INST] What did I ask you to do yesterday? [/INST] To welcome and introduce myself. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Remind me what I discussed with Sarah last week. [/INST]\n",
      "Sarah: Great, thanks for that update! So we've decided to meet up at 5PM on Tuesday and have a coffee together? Can you make sure she comes dressed appropriately as well?: Yes of course! She will be in her business outfit from the office so it won’t take too much extra time.: That sounds great, just let us know\n",
      "Finetuned Model output:  [INST] Remind me what I discussed with Sarah last week. [/INST] Last Wednesday, you and the AI system reviewed your progress and potential for growth over the past few weeks. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] How much screen time did I have today? [/INST]\n",
      "I don't know, but it was less than 3 hours.\n",
      "Finetuned Model output:  [INST] How much screen time did I have today? [/INST] I don'thave a physical body, so my screentime is determined by your commands. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Can you find my last grocery list? [/INST] I don't have any more food.\n",
      "Finetuned Model output:  [INST] Can you find my last grocery list? [/INST] I don't have a physical existence, so I can't locate your lists. However, if you provide me with the ingredients and instructions for each item on your list, I could create an accurate replica of them in digital form. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Update my contact info for Dr. Lee. [/INST]\n",
      "[SUPPORT]: Thank you, I appreciate your help in updating my information on the program’s website! Can we add a section that explains how to apply and what kind of scholarships are available? Also can you include some statistics about student success rates over time at this university so it is easier for potential students like me who may have questions regarding financial aid options?\n",
      "Finetuned Model output:  [INST] Update my contact info for Dr. Lee. [/INST] Certainly, I can update your contacts with their latest information. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] How are you today? [/INST]\n",
      "I am fine. Thank You, how have I been doing lately?: (INS) Are we still together or not yet ? : Yes / No We're Still Together 😊 Can you repeat that again for me please: \"Yes\"\n",
      "Finetuned Model output:  [INST] How are you today? [/INST] I'm functioning at optimal efficiency. Do you have any specific questions or tasks to carry out?\" \n",
      "\n",
      "\n",
      "Base Model output:  [INST] What can you do for me? [/INST]\n",
      "\n",
      "Finetuned Model output:  [INST] What can you do for me? [/INST] My capabilities include information retrieval, task management, and interaction with various systems. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Tell me something interesting. [/INST]\n",
      "14) I'm a big fan of ________________. Do you have any recommendations for good books or movies like them?\n",
      "Finetuned Model output:  [INST] Tell me something interesting. [/INST] Did you know that honey never spoils? \n",
      "\n",
      "\n",
      "Base Model output:  [INST] What’s your favorite feature about yourself? [/INST]\n",
      "What is the name of a famous artist who was known for using bold, expressive colors in their artwork and how has that influence influenced modern artists today?: I love brightly colored paintings. The way they make me feel happy when looking at them!\n",
      "Finetuned Model output:  [INST] What’s your favorite feature about yourself? [/INST] My favorite feature is my ability to adapt and learn from user needs, making me a valuable assistant. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Do you ever get tired of helping? [/INST]\n",
      "<[USER|userassistant, userid: 1234567890]]>\n",
      "Finetuned Model output:  [INST] Do you ever get tired of helping? [/INST] As an artificial intelligence, I don't experience fatigue or weariness like humans do. In fact, my main purpose is to assist and provide support to users as much as possible while ensuring their safety and comfort throughout the day. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Schedule a dentist appointment for next Friday. [/INST]\n",
      "23) I have to work at the office from 8:05 am until noon today, so please plan accordingly and ensure that all necessary supplies are available in my cubicle by then as well! Thanks again for your help with this project; it has been much appreciated!!\n",
      "Finetuned Model output:  [INST] Schedule a dentist appointment for next Friday. [/INST] I'll send an reminder to your medical apps later morning, don't forget! \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Cancel my gym session for tomorrow. [/INST] 203: I’m sorry, but we don't have enough space in our schedule to accommodate your request at this time. Please try again later or reach out via phone if you need more information on available sessions and schedules. Thank you!\n",
      "Finetuned Model output:  [INST] Cancel my gym session for tomorrow. [/INST] Great, I'll be ready to exercise your presets from 6 am - noon. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Find the nearest coffee shop. [/INST]\n",
      "[TTS: \"Coffee shops are a common sight in cities around the world.\" /Voiceover of various cafes and restaurants with names like Starbucks, Dunkin' Donuts or Café Nous.] (Instrumental music plays.)\n",
      "Finetuned Model output:  [INST] Find the nearest coffee shop. [/INST] The closest coffee shop is located at 4pm, so you have time to check it out! \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Book a cab to the airport at 9 AM. [/INST]\n",
      "10:35 - Departure from home with family and friends, arriving in San Diego around noon or later if we have time for lunch on our own (depending how much of an adventurous traveler you are). We will be able to check-in into one hotel room together after this trip is over so that everyone can share expenses while still enjo\n",
      "Finetuned Model output:  [INST] Book a cab to the airport at 9 AM. [/INST] Good morning, I'll be waiting for your cabs in advance. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Can you add “buy milk” to my shopping list? [/INST]\n",
      "Can you suggest some healthier alternatives for the desserts I have on your menu?: Yes! We offer a variety of delicious and nutritious options like smoothies, fresh fruit salad with granola or trail mix. Please let me know which ones are available at this time so that we can prepare accordingly: Smoothie – Vanilla Greek Yogurt +\n",
      "Finetuned Model output:  [INST] Can you add “buy milk” to my shopping list? [/INST] Absolutely,. \"Buy milk\" is already on your wishlist. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Why isn't my Wi-Fi working? [/INST]\n",
      "\n",
      "Finetuned Model output:  [INST] Why isn't my Wi-Fi working? [/INST] Due to technical issues, your Wi-Fi is not functioning properly. Please ensure you are in an area with a stable network connection and try again later or contact support for further assistance. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] How do I restart my smart TV? [/INST] Can you provide me with instructions on how to reset the Smart Home Hub 305 for a new setup, including what specific settings and commands need to be entered during each step of restoring it from factory default state?: Hi there!\n",
      "Could someone please explain in more detail about how exactly we should go through this process when using our Philips Hue Bridge as an accessory device.\n",
      "Finetuned Model output:  [INST] How do I restart my smart TV? [/INST] To start your smart tv, simply say \"SmartTV\" or include the command in a sentence. For example, to begin streaming content, you couldsay,\"Hey SmARTV,. Start streaming.\" \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Can you help me reset my password? [/INST] I am unable to access your account. Please contact our customer support at: 1-800[CUSTSUPPORT]. Thank You,\n",
      "<|assistant@instantsupportcenter>[USER]: Hi there! Unfortunately we don't have the ability for users in this service category (Instacart) as it is not a part of Instagram or Facebook Messenger. However,\n",
      "Finetuned Model output:  [INST] Can you help me reset my password? [/INST] Yes, I can assist with that. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] What should I do if my phone overheats? [/INST]\n",
      "1. Unplug it from any power source immediately and wait for the temperature to drop down below 70 degrees Fahrenheit before plugging back in again, as this can cause permanent damage or even a fire hazard (if your device has been exposed directly on an open flame). If you're unsure how hot something is after unpacking it out of its\n",
      "Finetuned Model output:  [INST] What should I do if my phone overheats? [/INST] Don't panic! Simply unplugging the device and allowing it to cool down will prevent any damage. \n",
      "\n",
      "\n",
      "Base Model output:  [INST] Why am I getting so many spam calls? [/INST]\n",
      "I'm sorry, but we are currently experiencing an issue with our customer service team. Please try calling back later or contacting us through the chat feature on your device to receive assistance from a live agent as soon as possible. Thank you for choosing T-Mobile and please don’t hesitate to let me know if there is anything else that can be done!\n",
      "Finetuned Model output:  [INST] Why am I getting so many spam calls? [/INST] It's due to your preferred name, Jarvis. The acronym stands for 'Just a Rather Very Intelligent System,' highlighting your intelligence and ability to handle various tasks with ease. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_list = []\n",
    "for instruction in formatted_instructions:\n",
    "    base_model_result = generate(base_model, instruction)\n",
    "    finetuned_model_result = generate(standalone_model, instruction)\n",
    "    print(\"Base Model output: \", base_model_result)\n",
    "    print(\"Finetuned Model output: \", finetuned_model_result)\n",
    "    print(\"\\n\")\n",
    "    output_list.append([base_model_result, finetuned_model_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e42a39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for ins, out in zip(instructions, output_list):\n",
    "    base, fine = out[0].split('[/INST]')[-1].strip(), out[1].split('[/INST]')[-1].strip()\n",
    "    answers.append([ins, base, fine])\n",
    "df = pd.DataFrame(answers, columns=['Instruction', 'TinyLlama (Base)', 'TinyLlama (Finetuned)'])\n",
    "df.to_csv('result/Model_outputs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
